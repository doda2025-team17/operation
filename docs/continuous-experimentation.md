# Continuous Experimentation

## Overview

This experiment evaluates a new version (v2) of the **app-service** that introduces an in-memory cache for SMS classification requests. The goal is to determine whether caching improves performance and reduces load on the model-service without negatively impacting correctness.

Two versions of both services are deployed simultaneously:

| Service | Stable (v1) | Canary (v2) |
|----------|-------------|--------------|
| app-service | No cache | TTL-based in-memory cache |
| model-service | Baseline | Identical logic (instrumented with version header) |

Istio DestinationRules and VirtualServices ensure **consistent routing**, so:
- app v1 always calls model v1
- app v2 always calls model v2
- mixed-version traffic (v1 → v2 or v2 → v1) is avoided

Traffic is split using canary routing.

---

## What Changed Compared to the Base Design

### App-service v2 Improvements

The following changes were introduced in v2:

#### 1. In-Memory Cache
- A thread-safe `ConcurrentHashMap` stores SMS predictions.
- Each entry has a **5-minute TTL**.
- Repeated requests for the same SMS bypass the model-service.
- Cache eviction happens automatically based on expiration time.

#### 2. Response Headers
Each response now includes:
- `X-App-Version`: identifies the running app version (v1 / v2)
- `X-Cache`: indicates whether the request was served from cache (`HIT` or `MISS`)

This allows validation directly using browser developer tools or curl.

#### 3. Additional Metrics
New Prometheus metrics were added:

| Metric | Description |
|--------|-------------|
| `sms_cache_hits_total` | Number of cache hits |
| `sms_cache_misses_total` | Number of cache misses |
| `sms_model_calls_total` | Number of calls made to model-service |
| `sms_request_latency_seconds` | End-to-end request latency |
| `sms_active_requests` | Concurrent in-flight requests |

All metrics include a `version` label (`v1` or `v2`) for comparison.

#### 4. Version Visibility in UI
The dashboard displays the active version (`Dashboard version: v1/v2`) to visually confirm routing behavior.

---

## Hypothesis

**H₀ (Null Hypothesis):**  
Introducing an in-memory cache in app-service v2 does not significantly reduce request latency or model-service load compared to v1.

**H₁ (Alternative Hypothesis):**  
App-service v2 reduces request latency and model-service calls compared to v1 for repeated SMS requests.

This hypothesis is falsifiable because it can be disproven if metrics show no meaningful improvement or regressions.

---

## Experiment Setup

### Deployment

- Both versions are deployed using Helm.
- Canary routing is enabled in Istio with a weighted split:
    - 90% → v1
    - 10% → v2
- Version labels (`version=v1`, `version=v2`) are used by DestinationRules.
- Routing guarantees consistent version pairing between app and model.

### Traffic Generation

Traffic is generated by repeatedly submitting identical SMS texts via:
- Browser UI
- or curl scripts

This creates cache opportunities in v2 while v1 always calls the model-service.

