# Continuous Experimentation

## Overview

This experiment evaluates a new version (v2) of the **app-service** that introduces an in-memory cache for SMS classification requests. The goal is to determine whether caching improves performance and reduces load on the model-service without negatively impacting correctness.

Two versions of both services are deployed simultaneously:

| Service | Stable (v1) | Canary (v2) |
|----------|-------------|--------------|
| app | No cache | TTL-based in-memory cache |
| model-service | Baseline | Identical logic (instrumented with version header) |

Istio DestinationRules and VirtualServices ensure **consistent routing**, so:
- app v1 always calls model v1
- app v2 always calls model v2
- mixed-version traffic (v1 → v2 or v2 → v1) is avoided

Traffic is split using canary routing.

---

## What Changed Compared to the Base Design

### App-service v2 Improvements

The following changes were introduced in v2:

#### 1. In-Memory Cache
- A thread-safe `ConcurrentHashMap` stores SMS predictions.
- Each entry has a **5-minute TTL**.
- Repeated requests for the same SMS bypass the model-service.
- Cache eviction happens automatically based on expiration time.

#### 2. Response Headers
Each response now includes:
- `X-App-Version`: identifies the running app version (v1 / v2)
- `X-Cache`: indicates whether the request was served from cache (`HIT` or `MISS`)

This allows validation directly using browser developer tools or curl.

#### 3. Additional Metrics
New Prometheus metrics were added:

| Metric | Description |
|--------|-------------|
| `sms_cache_hits_total` | Number of cache hits |
| `sms_cache_misses_total` | Number of cache misses |
| `sms_model_calls_total` | Number of calls made to model-service |
| `sms_request_latency_seconds` | End-to-end request latency |
| `sms_active_requests` | Concurrent in-flight requests |

All metrics include a `version` label (`v1` or `v2`) for comparison.

#### 4. Version Visibility in UI
The dashboard displays the active version (`Dashboard version: v1/v2`) to visually confirm routing behavior.

---

## Hypothesis

**H₀ (Null Hypothesis):**  
Introducing an in-memory cache in app-service v2 does not significantly reduce request latency or model-service load compared to v1.

**H₁ (Alternative Hypothesis):**  
App-service v2 reduces request latency and model-service calls compared to v1 for repeated SMS requests.

This hypothesis is falsifiable because it can be disproven if metrics show no meaningful improvement or regressions.

---

## Experiment Setup

### Deployment

- Both versions are deployed using Helm.
- Canary routing is enabled in Istio with a weighted split:
    - 90% → v1
    - 10% → v2
- Version labels (`version=v1`, `version=v2`) are used by DestinationRules.
- Routing guarantees consistent version pairing between app and model.

### Traffic Generation

Traffic is generated by repeatedly submitting identical SMS texts via:
- Browser UI
- or curl scripts

This creates cache opportunities in v2 while v1 always calls the model-service.

## Accessing Stable and Canary Versions

To enable deterministic testing of both versions, separate hostnames are configured through Istio routing:

- **Stable version (v1):**  
  `http://stable.sms-app.local`

- **Canary version (v2):**  
  `http://canary.sms-app.local`

Requests sent to these hostnames are routed explicitly to the corresponding application version using Istio VirtualService rules. This allows the evaluator to intentionally access and validate each version independently without relying on probabilistic traffic splitting.

In addition, weighted routing is enabled on the default host:

- 90% of traffic → stable (v1)
- 10% of traffic → canary (v2)

The active version can be verified using HTTP response headers (`X-App-Version`) or directly in the UI.

---

## Evaluation

The hypothesis is evaluated by comparing the behavior of the stable version (v1, no cache) and the canary version (v2, with cache) using Prometheus metrics visualized in Grafana. All metrics are labeled by `version`, enabling direct side-by-side comparison in dashboards.

The following metrics are used:

### Cache Effectiveness

- **`sms_cache_hits_total`** and **`sms_cache_misses_total`**
- The cache hit ratio indicates how often repeated SMS requests are served directly from memory in v2.
- A higher hit ratio in v2 demonstrates effective caching behavior.
- v1 is expected to have no cache hits.

### Model Load Reduction

- **`sms_model_calls_total`**
- Measures how many requests are forwarded to the model-service.
- A lower rate for v2 compared to v1 indicates that the cache successfully reduces backend load.

### Latency

- **`sms_request_latency_seconds` (p95)**
- Measures end-to-end request latency.
- v2 should exhibit lower or equal latency compared to v1, especially for repeated requests that are served from cache.

### Concurrency and Stability

- **`sms_active_requests`**
- Ensures that the cache does not introduce concurrency issues or request buildup.

---

### Decision Criteria

The canary version (v2) is accepted if all of the following conditions are satisfied:

1. **Cache utilization**  
   v2 shows a consistently non-zero cache hit rate while v1 remains at zero.

2. **Reduced model load**  
   v2 generates fewer model-service calls per minute than v1 for similar traffic patterns.

3. **Latency improvement or no regression**  
   The p95 latency of v2 is equal to or lower than v1 during steady traffic.

4. **Operational stability**  
   No increase in active requests, error rates, or abnormal behavior is observed for v2.

These criteria ensure that the cache improves performance without introducing regressions or instability.

---

## Results

**Figure X** shows the Grafana dashboard comparing stable (v1) and canary (v2).

*(Insert Grafana dashboard screenshot here)*

Observed metrics:

- **Cache Hits / Misses**  
  v2 shows increasing cache hits after repeated SMS requests, while v1 remains at zero.

- **Model Service Calls**  
  v2 generates fewer model calls per minute compared to v1, confirming that cached requests bypass the backend.

- **Latency (p95)**  
  The latency for v2 is consistently lower or equal to v1 for repeated requests.

- **Active Requests**  
  Both versions remain stable with no abnormal accumulation of in-flight requests.

---

## Interpretation and Decision

Based on the observed metrics, all decision criteria are satisfied:

- The cache in v2 achieves measurable cache hits.
- Model-service load is reduced for v2.
- No latency regressions are observed.
- System stability remains unaffected.

Therefore, the null hypothesis (H₀) is rejected and the alternative hypothesis (H₁) is accepted.  
The cached canary version (v2) demonstrates clear performance benefits and can be safely promoted to stable.

---

## Reflection

### Limitations

- Only 10% of traffic is routed to the canary, which slows down metric convergence and statistical confidence.
- Traffic generation was manual (browser and curl), which may not fully represent real production workloads.
- The experiment focuses mainly on latency and backend load; long-term memory usage and cache eviction behavior were not evaluated.

### Future Work

- Automate traffic generation using a load-testing tool to produce reproducible workloads.
- Experiment with different cache TTL values and cache sizes.
- Introduce additional metrics such as memory consumption and error rates.
- Gradually increase canary traffic percentage for higher confidence validation.

